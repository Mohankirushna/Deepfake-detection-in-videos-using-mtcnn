{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import cv2\n",
    "plt.style.use('ggplot')\n",
    "from IPython.display import Video\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_metadata = pd.read_json('../input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\n",
    "train_sample_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load metadata\n",
    "train_sample_metadata = pd.read_json('../input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\n",
    "\n",
    "# Display the first few rows of the metadata\n",
    "print(train_sample_metadata.head())\n",
    "\n",
    "# Visualize the distribution of fake vs. real videos\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=train_sample_metadata, x='label')\n",
    "plt.title('Distribution of Fake vs. Real Videos')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Define a function to display a frame from a video\n",
    "def display_video_frame(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if ret:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(frame)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Failed to read video: {video_path}\")\n",
    "\n",
    "# Path to the video folder\n",
    "video_folder = '../input/deepfake-detection-challenge/train_sample_videos/'\n",
    "\n",
    "# Display some sample frames from fake videos\n",
    "fake_videos = train_sample_metadata[train_sample_metadata['label'] == 'FAKE'].index\n",
    "print(\"Sample frames from Fake videos:\")\n",
    "for video in fake_videos[:3]:  # Display the first 3 fake videos\n",
    "    print(f\"Video: {video}\")\n",
    "    display_video_frame(os.path.join(video_folder, video))\n",
    "\n",
    "# Display some sample frames from real videos\n",
    "real_videos = train_sample_metadata[train_sample_metadata['label'] == 'REAL'].index\n",
    "print(\"Sample frames from Real videos:\")\n",
    "for video in real_videos[:3]:  # Display the first 3 real videos\n",
    "    print(f\"Video: {video}\")\n",
    "    display_video_frame(os.path.join(video_folder, video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mtcnn tensorflow opencv-python pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize MTCNN face detector\n",
    "detector = MTCNN()\n",
    "\n",
    "# Paths and setup\n",
    "video_folder = '../input/deepfake-detection-challenge/train_sample_videos/'\n",
    "metadata_path = '../input/deepfake-detection-challenge/train_sample_videos/metadata.json'\n",
    "\n",
    "train_sample_metadata = pd.read_json(metadata_path).T\n",
    "\n",
    "train_metadata, val_metadata = train_test_split(train_sample_metadata, test_size=0.2, random_state=42)\n",
    "\n",
    "class VideoFrameGenerator(Sequence):\n",
    "    def __init__(self, metadata, batch_size=32, target_size=(224, 224), shuffle=True):\n",
    "        self.metadata = metadata\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.metadata))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "               return int(np.ceil(len(self.metadata) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        batch_metadata = self.metadata.iloc[batch_indexes]\n",
    "        \n",
    "        X, y_labels = self.__data_generation(batch_metadata)\n",
    "        return X, y_labels\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __data_generation(self, batch_metadata):\n",
    "        X = []\n",
    "        y_labels = []\n",
    "        \n",
    "        for video_name, row in batch_metadata.iterrows():\n",
    "            video_path = os.path.join(video_folder, video_name)\n",
    "            label = 1 if row['label'] == 'FAKE' else 0\n",
    "            \n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            while cap.isOpened():\n",
    "                                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                faces = detector.detect_faces(frame_rgb)\n",
    "                \n",
    "                for face in faces:\n",
    "                    x, y, width, height = face['box']\n",
    "                    face_img = frame_rgb[y:y+height, x:x+width]\n",
    "                    face_img = cv2.resize(face_img, self.target_size)  # Resize face to target_size\n",
    "                    face_array = img_to_array(face_img) / 255.0  # Normalize pixel values\n",
    "                    \n",
    "                    X.append(face_array)\n",
    "                    y_labels.append(label)\n",
    "                    \n",
    "                    if len(X) >= self.batch_size:\n",
    "                        cap.release()\n",
    "                        return np.array(X), np.array(y_labels)\n",
    "            \n",
    "            cap.release()\n",
    "        \n",
    "        while len(X) < self.batch_size:\n",
    "            X.append(X[0])\n",
    "            y_labels.append(y_labels[0])\n",
    "        \n",
    "        return np.array(X), np.array(y_labels)\n",
    "\n",
    "batch_size = 32\n",
    "train_generator = VideoFrameGenerator(train_metadata, batch_size=batch_size)\n",
    "val_generator = VideoFrameGenerator(val_metadata, batch_size=batch_size)\n",
    "\n",
    "# Build the model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=2,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "# Evaluate the model \n",
    "loss, accuracy = model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "model_save_path = '/kaggle/working/deepfake_detection_model.h5'  \n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Initialize MTCNN face detector\n",
    "detector = MTCNN()\n",
    "\n",
    "# Load the trained model\n",
    "# model = load_model('path/to/your/trained_model.h5')  # Update with the actual path to your model\n",
    "\n",
    "# Function to detect and preprocess faces from a video\n",
    "def extract_faces_from_video(video_path, target_size=(224, 224)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    faces = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        detected_faces = detector.detect_faces(frame_rgb)\n",
    "\n",
    "        for face in detected_faces:\n",
    "            x, y, width, height = face['box']\n",
    "            face_img = frame_rgb[y:y+height, x:x+width]\n",
    "            face_img = cv2.resize(face_img, target_size)\n",
    "            face_array = img_to_array(face_img) / 255.0\n",
    "            faces.append(face_array)\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(faces)\n",
    "\n",
    "# Function to predict if the video is fake or real\n",
    "def predict_video(video_path):\n",
    "    faces = extract_faces_from_video(video_path)\n",
    "    if len(faces) == 0:\n",
    "                print(\"No faces detected in the video.\")\n",
    "        return None\n",
    "\n",
    "    predictions = model.predict(faces)\n",
    "    avg_prediction = np.mean(predictions)\n",
    "\n",
    "    if avg_prediction > 0.5:\n",
    "        print(f\"The video '{video_path}' is predicted to be FAKE.\")\n",
    "    else:\n",
    "        print(f\"The video '{video_path}' is predicted to be REAL.\")\n",
    "\n",
    "    return avg_prediction\n",
    "\n",
    "# Test the prediction function with a sample video\n",
    "video_path = '/kaggle/input/deepfake-detection-challenge/test_videos/aassnaulhq.mp4'  # Update with the actual path to the test video\n",
    "predict_video(video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
